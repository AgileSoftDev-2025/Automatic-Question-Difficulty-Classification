# -*- coding: utf-8 -*-
"""Bloomers-main

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z9e2RNw5V_5RTZaFMKMz_XkEnxReAU-r

Import Library
"""

# Install required packages
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install transformers datasets pandas numpy scikit-learn tqdm

import os
import numpy as np
import pandas as pd
import torch
from torch.nn.functional import sigmoid
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score
import warnings
from google.colab import files
import io

from transformers import (
    AutoTokenizer,
    AutoConfig,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
    set_seed,
)
from datasets import Dataset

# Suppress warnings
warnings.filterwarnings("ignore")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['WANDB_SILENT'] = 'true'  # Disable wandb logging

# ---------------- Settings ----------------
TEXT_COLUMN = "Learning_outcome"  # Updated to match your CSV
LABEL_COLUMNS = ["Remember", "Understand", "Apply", "Analyze", "Evaluate", "Create"]  # Updated to match your CSV
PRETRAINED_MODEL = "roberta-base"
OUTPUT_DIR = "roberta_multilabel_output"
RANDOM_SEED = 42
BATCH_SIZE = 16
EPOCHS = 3
LEARNING_RATE = 2e-5
THRESHOLD = 0.5
# ------------------------------------------

set_seed(RANDOM_SEED)

def upload_csv_file():
    """Upload CSV file using Google Colab file uploader"""
    print("=== Upload Your CSV File ===")
    print("Please select your CSV file that contains:")
    print(f"- Text column named: '{TEXT_COLUMN}' (or 'Learning_outcome')")
    print(f"- Label columns: {LABEL_COLUMNS}")
    print("- Or we'll try to auto-detect column names")
    print("\nClick 'Choose Files' button below:")

    uploaded = files.upload()

    if not uploaded:
        raise ValueError("No file uploaded!")

    # Get the first uploaded file
    filename = list(uploaded.keys())[0]

    # Save the file
    with open(filename, 'wb') as f:
        f.write(uploaded[filename])

    print(f"âœ“ File '{filename}' uploaded successfully!")
    return filename

def read_data(csv_path: str) -> pd.DataFrame:
    """Read and validate CSV data with automatic column detection"""
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"Dataset not found: {csv_path}")

    # Try to read with different encodings
    encodings = ['utf-8', 'latin-1', 'cp1252']
    df = None

    for encoding in encodings:
        try:
            df = pd.read_csv(csv_path, encoding=encoding)
            print(f"âœ“ CSV loaded successfully with {encoding} encoding")
            break
        except UnicodeDecodeError:
            continue

    if df is None:
        raise ValueError("Could not read CSV with any encoding")

    print(f"Dataset shape: {df.shape}")
    print(f"Columns: {list(df.columns)}")

    # Auto-detect text column
    global TEXT_COLUMN, LABEL_COLUMNS

    # Check for common text column names
    possible_text_cols = ['Learning_outcome', 'text', 'Text', 'content', 'Content', 'sentence', 'description']
    detected_text_col = None

    for col in possible_text_cols:
        if col in df.columns:
            detected_text_col = col
            break

    if detected_text_col:
        TEXT_COLUMN = detected_text_col
        print(f"âœ“ Auto-detected text column: '{TEXT_COLUMN}'")
    else:
        # Show available columns and ask user to modify
        print(f"âŒ Could not find text column. Available columns: {list(df.columns)}")
        raise ValueError(f"Please ensure your CSV has one of these text columns: {possible_text_cols}")

    # Auto-detect label columns (look for columns with binary values 0/1)
    detected_label_cols = []
    for col in df.columns:
        if col != TEXT_COLUMN:
            # Check if column contains only 0s and 1s (after filling NaN)
            col_values = df[col].fillna(0)
            if col_values.dtype in ['int64', 'float64'] and set(col_values.unique()).issubset({0, 1, 0.0, 1.0}):
                detected_label_cols.append(col)

    if detected_label_cols:
        LABEL_COLUMNS = detected_label_cols
        print(f"âœ“ Auto-detected label columns: {LABEL_COLUMNS}")
    else:
        print(f"âŒ Could not detect binary label columns. Available columns: {list(df.columns)}")
        # Try with the original column names anyway
        missing_labels = [col for col in LABEL_COLUMNS if col not in df.columns]
        if missing_labels:
            raise ValueError(f"Missing label columns: {missing_labels}")

    # Fill NaN values and convert to int
    df[LABEL_COLUMNS] = df[LABEL_COLUMNS].fillna(0).astype(int)

    # Remove rows where text is empty or NaN
    df = df.dropna(subset=[TEXT_COLUMN])
    df = df[df[TEXT_COLUMN].str.strip() != '']

    print(f"Dataset loaded: {len(df)} samples after cleaning")
    print(f"Label distribution:")
    for col in LABEL_COLUMNS:
        positive_count = df[col].sum()
        percentage = (positive_count / len(df)) * 100
        print(f"  {col}: {positive_count} positive samples ({percentage:.1f}%)")

    return df

def prepare_labels(examples):
    """Convert label columns to a single labels list with correct dtype"""
    labels = []
    for i in range(len(examples[TEXT_COLUMN])):
        label_row = [float(examples[col][i]) for col in LABEL_COLUMNS]  # Convert to float
        labels.append(label_row)
    return {"labels": labels}

def tokenize_function(examples, tokenizer):
    """Tokenize text data"""
    # Get the actual text column name (it might change during auto-detection)
    text_key = TEXT_COLUMN if TEXT_COLUMN in examples else "Learning_outcome"
    return tokenizer(
        examples[text_key],
        truncation=True,
        padding=False,  # Let DataCollator handle padding
        max_length=512
    )

def compute_metrics(eval_pred):
    """Compute metrics for multi-label classification"""
    predictions, labels = eval_pred

    # Apply sigmoid to get probabilities
    sigmoid_predictions = sigmoid(torch.tensor(predictions)).numpy()

    # Apply threshold to get binary predictions
    binary_predictions = (sigmoid_predictions >= THRESHOLD).astype(int)

    # Ensure labels are in the correct format
    if isinstance(labels, list):
        labels = np.array(labels)

    results = {}

    # Compute overall metrics
    results["f1_micro"] = f1_score(labels, binary_predictions, average="micro", zero_division=0)
    results["f1_macro"] = f1_score(labels, binary_predictions, average="macro", zero_division=0)
    results["precision_micro"] = precision_score(labels, binary_predictions, average="micro", zero_division=0)
    results["recall_micro"] = recall_score(labels, binary_predictions, average="micro", zero_division=0)

    # Compute per-label F1 scores
    per_label_f1 = f1_score(labels, binary_predictions, average=None, zero_division=0)
    for i, col in enumerate(LABEL_COLUMNS):
        results[f"f1_{col}"] = float(per_label_f1[i]) if i < len(per_label_f1) else 0.0

    # Compute ROC AUC scores (only if we have both positive and negative samples)
    try:
        # Check if we have both classes for each label
        valid_labels = []
        valid_probs = []
        valid_label_indices = []

        for i in range(labels.shape[1]):
            if len(np.unique(labels[:, i])) > 1:  # Both 0 and 1 present
                valid_labels.append(labels[:, i])
                valid_probs.append(sigmoid_predictions[:, i])
                valid_label_indices.append(i)

        if valid_labels:
            valid_labels = np.column_stack(valid_labels)
            valid_probs = np.column_stack(valid_probs)

            roc_auc_per_label = roc_auc_score(valid_labels, valid_probs, average=None)
            results["rocauc_macro"] = float(roc_auc_score(valid_labels, valid_probs, average="macro"))

            # Map back to original label indices
            for idx, label_idx in enumerate(valid_label_indices):
                col = LABEL_COLUMNS[label_idx]
                results[f"rocauc_{col}"] = float(roc_auc_per_label[idx])

    except Exception as e:
        print(f"Warning: Could not compute ROC AUC: {e}")

    return results

def main():
    print("=== Multi-label Classification Training ===")
    print("This script will train a RoBERTa model for multi-label text classification")
    print("using Bloom's Taxonomy categories.\n")

    # Upload and load data
    try:
        csv_file = upload_csv_file()
        df = read_data(csv_file)
    except Exception as e:
        print(f"Error loading data: {e}")
        return

    # Check if we have enough data
    if len(df) < 10:
        print(f"Error: Not enough data ({len(df)} samples). Need at least 10 samples.")
        return

    # Split data
    test_size = min(0.20, max(0.1, 10/len(df)))  # At least 10% or 10 samples for test
    val_size = min(0.125, max(0.05, 5/len(df)))  # At least 5% or 5 samples for validation

    train_df, test_df = train_test_split(
        df, test_size=test_size, random_state=RANDOM_SEED, shuffle=True
    )

    if len(train_df) > 10:
        train_df, val_df = train_test_split(
            train_df, test_size=val_size/(1-test_size), random_state=RANDOM_SEED, shuffle=True
        )
    else:
        val_df = test_df  # Use test set as validation if train set is too small

    print(f"\nData split:")
    print(f"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}")

    # Create datasets
    train_dataset = Dataset.from_pandas(train_df[[TEXT_COLUMN] + LABEL_COLUMNS].reset_index(drop=True))
    val_dataset = Dataset.from_pandas(val_df[[TEXT_COLUMN] + LABEL_COLUMNS].reset_index(drop=True))
    test_dataset = Dataset.from_pandas(test_df[[TEXT_COLUMN] + LABEL_COLUMNS].reset_index(drop=True))

    # Initialize tokenizer
    print(f"\nLoading tokenizer: {PRETRAINED_MODEL}")
    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)

    # Add labels to datasets
    print("Preparing datasets...")
    train_dataset = train_dataset.map(prepare_labels, batched=True)
    val_dataset = val_dataset.map(prepare_labels, batched=True)
    test_dataset = test_dataset.map(prepare_labels, batched=True)

    # Tokenize datasets
    train_dataset = train_dataset.map(
        lambda examples: tokenize_function(examples, tokenizer),
        batched=True,
        remove_columns=[TEXT_COLUMN] + LABEL_COLUMNS
    )
    val_dataset = val_dataset.map(
        lambda examples: tokenize_function(examples, tokenizer),
        batched=True,
        remove_columns=[TEXT_COLUMN] + LABEL_COLUMNS
    )
    test_dataset = test_dataset.map(
        lambda examples: tokenize_function(examples, tokenizer),
        batched=True,
        remove_columns=[TEXT_COLUMN] + LABEL_COLUMNS
    )

    # Set format for torch with proper dtypes
    train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
    val_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
    test_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

    # Ensure labels are float tensors for multi-label classification
    def convert_labels_to_float(batch):
        if 'labels' in batch:
            batch['labels'] = batch['labels'].float()
        return batch

    train_dataset = train_dataset.map(convert_labels_to_float, batched=True)
    val_dataset = val_dataset.map(convert_labels_to_float, batched=True)
    test_dataset = test_dataset.map(convert_labels_to_float, batched=True)

    # Load model
    print(f"Loading model: {PRETRAINED_MODEL}")
    config = AutoConfig.from_pretrained(
        PRETRAINED_MODEL,
        num_labels=len(LABEL_COLUMNS),
        problem_type="multi_label_classification",
        id2label={i: label for i, label in enumerate(LABEL_COLUMNS)},
        label2id={label: i for i, label in enumerate(LABEL_COLUMNS)}
    )

    model = AutoModelForSequenceClassification.from_pretrained(
        PRETRAINED_MODEL,
        config=config
    )

    # Data collator
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="pt")

    # Adjust batch size based on available data
    adjusted_batch_size = min(BATCH_SIZE, len(train_dataset) // 2, 8)

    # Training arguments
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        eval_strategy="epoch",  # Updated parameter name
        save_strategy="epoch",
        learning_rate=LEARNING_RATE,
        per_device_train_batch_size=adjusted_batch_size,
        per_device_eval_batch_size=adjusted_batch_size,
        num_train_epochs=EPOCHS,
        weight_decay=0.01,
        load_best_model_at_end=True,
        metric_for_best_model="f1_micro",
        greater_is_better=True,
        save_total_limit=2,
        seed=RANDOM_SEED,
        fp16=torch.cuda.is_available(),
        dataloader_pin_memory=False,
        report_to=[],  # Disable wandb completely
        logging_dir=f"{OUTPUT_DIR}/logs",
        logging_steps=max(1, len(train_dataset) // (adjusted_batch_size * 4)),
        remove_unused_columns=True,
        dataloader_num_workers=0,  # Important for Colab
    )

    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    # Train model
    print(f"\n=== Starting Training ===")
    print(f"Model: {PRETRAINED_MODEL}")
    print(f"Epochs: {EPOCHS}")
    print(f"Batch size: {adjusted_batch_size}")
    print(f"Learning rate: {LEARNING_RATE}")
    print("=" * 50)

    try:
        train_result = trainer.train()

        # Evaluate on test set
        print("\n=== Evaluating on Test Set ===")
        test_results = trainer.evaluate(eval_dataset=test_dataset)

        print("\nFinal Test Results:")
        print("-" * 40)
        for key, value in sorted(test_results.items()):
            if key.startswith(('eval_f1', 'eval_precision', 'eval_recall', 'eval_rocauc')):
                print(f"{key.replace('eval_', '').upper()}: {value:.4f}")

        # Save model
        print(f"\nSaving model to {OUTPUT_DIR}...")
        trainer.save_model(OUTPUT_DIR)
        tokenizer.save_pretrained(OUTPUT_DIR)

        # Save training info
        import json
        train_info = {
            'model_name': PRETRAINED_MODEL,
            'num_labels': len(LABEL_COLUMNS),
            'label_columns': LABEL_COLUMNS,
            'threshold': THRESHOLD,
            'train_samples': len(train_dataset),
            'val_samples': len(val_dataset),
            'test_samples': len(test_dataset),
            'epochs': EPOCHS,
            'batch_size': adjusted_batch_size,
            'learning_rate': LEARNING_RATE,
            'final_test_results': test_results
        }

        with open(f"{OUTPUT_DIR}/training_info.json", "w") as f:
            json.dump(train_info, f, indent=2)

        print("\n" + "="*50)
        print("ðŸŽ‰ TRAINING COMPLETED SUCCESSFULLY! ðŸŽ‰")
        print("="*50)
        print(f"âœ“ Model saved to: {OUTPUT_DIR}")
        print(f"âœ“ Training info saved to: {OUTPUT_DIR}/training_info.json")

        # Download results
        print("\n=== Download Results ===")
        print("You can download the trained model and results:")

        try:
            import shutil
            shutil.make_archive(OUTPUT_DIR, 'zip', OUTPUT_DIR)
            files.download(f"{OUTPUT_DIR}.zip")
            print(f"âœ“ Results downloaded as {OUTPUT_DIR}.zip")
        except:
            print("To download results manually:")
            print(f"1. Go to Files tab in Colab")
            print(f"2. Navigate to {OUTPUT_DIR} folder")
            print(f"3. Download the files you need")

    except Exception as e:
        print(f"âŒ Training failed with error: {e}")
        import traceback
        traceback.print_exc()

# Function to make predictions on new text
def predict_text(text, model_path=OUTPUT_DIR):
    """Make predictions on new text using trained model"""
    try:
        # Load model and tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)

        # Tokenize text
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)

        # Make prediction
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = sigmoid(outputs.logits).numpy()[0]

        # Apply threshold and create results
        results = {}
        for i, label in enumerate(LABEL_COLUMNS):
            prob = float(predictions[i])
            predicted = prob >= THRESHOLD
            results[label] = {
                'probability': prob,
                'predicted': predicted
            }

        return results

    except Exception as e:
        print(f"Error making prediction: {e}")
        return None

# Run main function
if __name__ == "__main__":
    main()